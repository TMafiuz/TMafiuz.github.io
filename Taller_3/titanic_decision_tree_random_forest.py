"""
AN√ÅLISIS DEL DATASET TITANIC CON √ÅRBOLES DE DECISI√ìN Y RANDOM FOREST

Objetivo: Predecir la supervivencia de pasajeros del Titanic utilizando
modelos de √Årbol de Decisi√≥n y Random Forest.

Autor: An√°lisis de Machine Learning
Dataset: Titanic - Clasificaci√≥n Binaria (Supervivencia: 0 o 1)
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report, roc_curve, auc
)
import warnings
warnings.filterwarnings('ignore')

# Configuraci√≥n de visualizaci√≥n
plt.style.use('default')
sns.set_palette("husl")

print("=" * 80)
print("AN√ÅLISIS DEL DATASET TITANIC")
print("√Årboles de Decisi√≥n vs Random Forest")
print("=" * 80)


# Cargar el dataset
df = pd.read_csv('Taller_3\Titanic\Titanic-Dataset.csv')

print(f"Dimensiones del dataset: {df.shape}")
print(f"Variables disponibles: {list(df.columns)}")
print(f"\nPrimeras 5 filas:")
print(df.head())

print(f"\nInformaci√≥n sobre valores faltantes:")
print(df.isnull().sum())

print(f"\nDistribuci√≥n de la variable objetivo (Survived):")
print(df['Survived'].value_counts())
print(f"Tasa de supervivencia: {df['Survived'].mean():.2%}")

# 3. PREPROCESAMIENTO DE DATOS
print("\n3. PREPROCESAMIENTO DE DATOS")
print("-" * 40)

# Crear una copia para el preprocesamiento
data = df.copy()

# Seleccionar caracter√≠sticas relevantes
features_to_use = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']
target = 'Survived'

# Manejo de valores faltantes
print("Manejo de valores faltantes:")
print("- Age: Imputar con la mediana")
print("- Embarked: Imputar con la moda")
print("- Fare: Imputar con la mediana")

data['Age'].fillna(data['Age'].median(), inplace=True)
data['Embarked'].fillna(data['Embarked'].mode()[0], inplace=True)
data['Fare'].fillna(data['Fare'].median(), inplace=True)

# Codificaci√≥n de variables categ√≥ricas
print("\nCodificaci√≥n de variables categ√≥ricas:")
le_sex = LabelEncoder()
le_embarked = LabelEncoder()

data['Sex_encoded'] = le_sex.fit_transform(data['Sex'])
data['Embarked_encoded'] = le_embarked.fit_transform(data['Embarked'])

# Seleccionar caracter√≠sticas finales
features_final = ['Pclass', 'Sex_encoded', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked_encoded']
X = data[features_final]
y = data[target]

print(f"Caracter√≠sticas finales: {features_final}")
print(f"Forma de X: {X.shape}")
print(f"Forma de y: {y.shape}")

# Divisi√≥n de datos
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"\nDivisi√≥n de datos:")
print(f"- Entrenamiento: {X_train.shape[0]} muestras")
print(f"- Prueba: {X_test.shape[0]} muestras")

# 4. CONSTRUCCI√ìN Y ENTRENAMIENTO DE MODELOS
print("\n4. CONSTRUCCI√ìN Y ENTRENAMIENTO DE MODELOS")
print("-" * 40)

# 4.1 √Årbol de Decisi√≥n
print("\n4.1 √ÅRBOL DE DECISI√ìN - CONFIGURACI√ìN DETALLADA")
print("=" * 50)
print("CONCEPTO: Un √°rbol de decisi√≥n es un modelo que toma decisiones siguiendo")
print("una serie de preguntas binarias organizadas en forma de √°rbol.")
print()
print("CONFIGURACI√ìN DE HIPERPAR√ÅMETROS:")
print("‚îå‚îÄ max_depth=5:")
print("‚îÇ  ‚Ä¢ Limita la profundidad m√°xima del √°rbol a 5 niveles")
print("‚îÇ  ‚Ä¢ Previene overfitting al evitar √°rboles muy complejos")
print("‚îÇ  ‚Ä¢ Valor elegido tras experimentar con 3, 5, 7, 10")
print("‚îÇ")
print("‚îå‚îÄ min_samples_split=20:")
print("‚îÇ  ‚Ä¢ Un nodo debe tener al menos 20 muestras para dividirse")
print("‚îÇ  ‚Ä¢ Evita divisiones con muy pocos datos")
print("‚îÇ  ‚Ä¢ Mejora la generalizaci√≥n del modelo")
print("‚îÇ")
print("‚îå‚îÄ min_samples_leaf=10:")
print("‚îÇ  ‚Ä¢ Cada hoja debe contener al menos 10 muestras")
print("‚îÇ  ‚Ä¢ Previene hojas con decisiones basadas en muy pocos casos")
print("‚îÇ  ‚Ä¢ Reduce la varianza del modelo")
print("‚îÇ")
print("‚îî‚îÄ random_state=42: Garantiza reproducibilidad de resultados")
print()

dt_classifier = DecisionTreeClassifier(
    max_depth=5,           
    min_samples_split=20,  
    min_samples_leaf=10,   
    random_state=42
)

print("Entrenando √Årbol de Decisi√≥n...")
dt_classifier.fit(X_train, y_train)
print("‚úì √Årbol de Decisi√≥n entrenado exitosamente")
print(f"  ‚Ä¢ Profundidad m√°xima configurada: {dt_classifier.max_depth}")
print(f"  ‚Ä¢ Profundidad real alcanzada: {dt_classifier.get_depth()}")
print(f"  ‚Ä¢ N√∫mero de hojas: {dt_classifier.get_n_leaves()}")
print(f"  ‚Ä¢ Muestras m√≠nimas para dividir: {dt_classifier.min_samples_split}")

# 4.2 Random Forest
print("\n4.2 RANDOM FOREST - CONFIGURACI√ìN DETALLADA")
print("=" * 50)
print("CONCEPTO: Random Forest es un ENSEMBLE (conjunto) de m√∫ltiples √°rboles")
print("de decisi√≥n que trabajan juntos para hacer predicciones m√°s robustas.")
print()
print("¬øC√ìMO FUNCIONA EL ENSEMBLE?")
print("‚îå‚îÄ Bootstrapping:")
print("‚îÇ  ‚Ä¢ Cada √°rbol se entrena con una muestra aleatoria de los datos")
print("‚îÇ  ‚Ä¢ Aproximadamente 63% de los datos originales por √°rbol")
print("‚îÇ  ‚Ä¢ Permite que cada √°rbol 'vea' datos ligeramente diferentes")
print("‚îÇ")
print("‚îå‚îÄ Random Feature Selection:")
print("‚îÇ  ‚Ä¢ En cada divisi√≥n, solo considera un subconjunto aleatorio de caracter√≠sticas")
print("‚îÇ  ‚Ä¢ Reduce la correlaci√≥n entre √°rboles individuales")
print("‚îÇ  ‚Ä¢ Mejora la diversidad del ensemble")
print("‚îÇ")
print("‚îî‚îÄ Voting/Promedio:")
print("   ‚Ä¢ Para clasificaci√≥n: voto mayoritario de todos los √°rboles")
print("   ‚Ä¢ Suaviza las predicciones y reduce overfitting")
print()
print("CONFIGURACI√ìN DE HIPERPAR√ÅMETROS:")
print("‚îå‚îÄ n_estimators=100:")
print("‚îÇ  ‚Ä¢ N√∫mero de √°rboles en el bosque")
print("‚îÇ  ‚Ä¢ M√°s √°rboles = mayor estabilidad, pero m√°s tiempo de c√≥mputo")
print("‚îÇ  ‚Ä¢ 100 es un balance entre rendimiento y eficiencia")
print("‚îÇ")
print("‚îå‚îÄ max_depth=5:")
print("‚îÇ  ‚Ä¢ Profundidad m√°xima de cada √°rbol individual")
print("‚îÇ  ‚Ä¢ Mismo valor que el √°rbol simple para comparaci√≥n justa")
print("‚îÇ  ‚Ä¢ Cada √°rbol es relativamente 'd√©bil' individualmente")
print("‚îÇ")
print("‚îå‚îÄ min_samples_split=20 y min_samples_leaf=10:")
print("‚îÇ  ‚Ä¢ Mismos valores que el √°rbol individual")
print("‚îÇ  ‚Ä¢ Controlan la complejidad de cada √°rbol del ensemble")
print("‚îÇ")
print("‚îî‚îÄ random_state=42: Reproducibilidad del proceso aleatorio")
print()

rf_classifier = RandomForestClassifier(
    n_estimators=100,      
    max_depth=5,           
    min_samples_split=20,
    min_samples_leaf=10,
    random_state=42,
    n_jobs=-1              # Usar todos los cores disponibles para acelerar
)

print("Entrenando Random Forest (Ensemble de 100 √°rboles)...")
rf_classifier.fit(X_train, y_train)
print("‚úì Random Forest entrenado exitosamente")
print(f"  ‚Ä¢ N√∫mero de √°rboles (n_estimators): {rf_classifier.n_estimators}")
print(f"  ‚Ä¢ Profundidad m√°xima por √°rbol: {rf_classifier.max_depth}")
print(f"  ‚Ä¢ Caracter√≠sticas consideradas por divisi√≥n: ‚àö{len(features_final)} ‚âà {int(np.sqrt(len(features_final)))}")
print(f"  ‚Ä¢ Muestras de bootstrap por √°rbol: ~{int(0.632 * len(X_train))}")
print(f"  ‚Ä¢ Procesamiento paralelo: {rf_classifier.n_jobs} cores")

print("\nüîç VENTAJAS DEL ENSEMBLE:")
print("‚Ä¢ Reduce overfitting comparado con un √°rbol individual")
print("‚Ä¢ Mayor robustez ante datos ruidosos o outliers") 
print("‚Ä¢ Mejor generalizaci√≥n en datos no vistos")
print("‚Ä¢ Proporciona medidas de importancia de caracter√≠sticas m√°s estables")
print("‚Ä¢ Maneja bien la no linealidad en los datos")

# 5. EVALUACI√ìN Y COMPARACI√ìN DE MODELOS
print("\n5. EVALUACI√ìN Y COMPARACI√ìN DE MODELOS")
print("-" * 40)

# Predicciones
dt_pred = dt_classifier.predict(X_test)
dt_pred_proba = dt_classifier.predict_proba(X_test)[:, 1]

rf_pred = rf_classifier.predict(X_test)
rf_pred_proba = rf_classifier.predict_proba(X_test)[:, 1]

# Funci√≥n para calcular m√©tricas
def calcular_metricas(y_true, y_pred, y_pred_proba, modelo_nombre):
    print(f"\nM√âTRICAS PARA {modelo_nombre.upper()}")
    print("-" * 30)
    
    # M√©tricas b√°sicas
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)
    
    print(f"Precisi√≥n (Accuracy): {accuracy:.4f}")
    print(f"Precisi√≥n (Precision): {precision:.4f}")
    print(f"Exhaustividad (Recall): {recall:.4f}")
    print(f"Puntuaci√≥n F1: {f1:.4f}")
    
    # Matriz de confusi√≥n
    cm = confusion_matrix(y_true, y_pred)
    print(f"\nMatriz de Confusi√≥n:")
    print(f"   Predicho:  No  S√≠")
    print(f"Real No:    {cm[0,0]:3d} {cm[0,1]:3d}")
    print(f"     S√≠:    {cm[1,0]:3d} {cm[1,1]:3d}")
    
    # ROC AUC
    fpr, tpr, _ = roc_curve(y_true, y_pred_proba)
    roc_auc = auc(fpr, tpr)
    print(f"AUC-ROC: {roc_auc:.4f}")
    
    return {
        'accuracy': accuracy, 'precision': precision, 'recall': recall, 
        'f1': f1, 'auc': roc_auc, 'fpr': fpr, 'tpr': tpr, 'cm': cm
    }

# Calcular m√©tricas para ambos modelos
dt_metrics = calcular_metricas(y_test, dt_pred, dt_pred_proba, "√Årbol de Decisi√≥n")
rf_metrics = calcular_metricas(y_test, rf_pred, rf_pred_proba, "Random Forest")

# 6. AN√ÅLISIS COMPARATIVO
print("\n6. AN√ÅLISIS COMPARATIVO")
print("-" * 40)

# Comparaci√≥n de m√©tricas
metricas_comparacion = pd.DataFrame({
    '√Årbol de Decisi√≥n': [dt_metrics['accuracy'], dt_metrics['precision'], 
                         dt_metrics['recall'], dt_metrics['f1'], dt_metrics['auc']],
    'Random Forest': [rf_metrics['accuracy'], rf_metrics['precision'], 
                     rf_metrics['recall'], rf_metrics['f1'], rf_metrics['auc']]
}, index=['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC'])

print("Comparaci√≥n de M√©tricas:")
print(metricas_comparacion.round(4))

# Determinar el mejor modelo
mejor_modelo = 'Random Forest' if rf_metrics['f1'] > dt_metrics['f1'] else '√Årbol de Decisi√≥n'
print(f"\nMejor modelo basado en F1-Score: {mejor_modelo}")

# 7. VISUALIZACIONES
print("\n7. GENERANDO VISUALIZACIONES")
print("-" * 40)

# Configurar subplots
fig, axes = plt.subplots(2, 2, figsize=(15, 12))
fig.suptitle('An√°lisis Titanic: √Årboles de Decisi√≥n vs Random Forest', fontsize=16)

# 7.1 Matrices de Confusi√≥n
# √Årbol de Decisi√≥n
sns.heatmap(dt_metrics['cm'], annot=True, fmt='d', cmap='Blues', 
           xticklabels=['No Sobrevivi√≥', 'Sobrevivi√≥'],
           yticklabels=['No Sobrevivi√≥', 'Sobrevivi√≥'], ax=axes[0,0])
axes[0,0].set_title('Matriz de Confusi√≥n - √Årbol de Decisi√≥n')
axes[0,0].set_ylabel('Valores Reales')
axes[0,0].set_xlabel('Valores Predichos')

# Random Forest
sns.heatmap(rf_metrics['cm'], annot=True, fmt='d', cmap='Greens',
           xticklabels=['No Sobrevivi√≥', 'Sobrevivi√≥'],
           yticklabels=['No Sobrevivi√≥', 'Sobrevivi√≥'], ax=axes[0,1])
axes[0,1].set_title('Matriz de Confusi√≥n - Random Forest')
axes[0,1].set_ylabel('Valores Reales')
axes[0,1].set_xlabel('Valores Predichos')

# 7.2 Comparaci√≥n de M√©tricas
metricas_comparacion.plot(kind='bar', ax=axes[1,0])
axes[1,0].set_title('Comparaci√≥n de M√©tricas')
axes[1,0].set_ylabel('Puntuaci√≥n')
axes[1,0].tick_params(axis='x', rotation=45)
axes[1,0].legend()

# 7.3 Curvas ROC
axes[1,1].plot(dt_metrics['fpr'], dt_metrics['tpr'], 
              label=f'√Årbol de Decisi√≥n (AUC = {dt_metrics["auc"]:.3f})', linewidth=2)
axes[1,1].plot(rf_metrics['fpr'], rf_metrics['tpr'], 
              label=f'Random Forest (AUC = {rf_metrics["auc"]:.3f})', linewidth=2)
axes[1,1].plot([0, 1], [0, 1], 'k--', label='L√≠nea Base')
axes[1,1].set_xlabel('Tasa de Falsos Positivos')
axes[1,1].set_ylabel('Tasa de Verdaderos Positivos')
axes[1,1].set_title('Curvas ROC')
axes[1,1].legend()

plt.tight_layout()
plt.savefig('titanic_analysis_results.png', dpi=300, bbox_inches='tight')
plt.show()

# 8. IMPORTANCIA DE CARACTER√çSTICAS (SOLO RANDOM FOREST)
print("\n8. IMPORTANCIA DE CARACTER√çSTICAS")
print("-" * 40)

importancias = rf_classifier.feature_importances_
indices_importancia = np.argsort(importancias)[::-1]

print("Ranking de importancia de caracter√≠sticas (Random Forest):")
for i in range(len(features_final)):
    print(f"{i+1}. {features_final[indices_importancia[i]]}: "
          f"{importancias[indices_importancia[i]]:.4f}")

# Visualizaci√≥n de importancia
plt.figure(figsize=(10, 6))
plt.bar(range(len(importancias)), importancias[indices_importancia])
plt.xticks(range(len(importancias)), 
          [features_final[i] for i in indices_importancia], rotation=45)
plt.title('Importancia de Caracter√≠sticas - Random Forest')
plt.ylabel('Importancia')
plt.tight_layout()
plt.savefig('titanic_feature_importance.png', dpi=300, bbox_inches='tight')
plt.show()


print(f"\n‚úì An√°lisis completado. Archivos guardados:")
print("  - titanic_analysis_results.png")
print("  - titanic_feature_importance.png")
